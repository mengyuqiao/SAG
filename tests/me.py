import torch
import time
import os
from transformers import AutoModelForCausalLM, AutoTokenizer, TextStreamer

# è„šæœ¬ï¼šclinicalnlplab/me-llama
MODEL_ID = "clinicalnlplab/me-llama" 

def run_mellama_benchmark(question):
    print(f"\n[ğŸ¥] Starting Rigorous Me-LLaMA-70B Benchmark")
    
    # 1. ç¯å¢ƒæ£€æŸ¥ï¼šç¡®ä¿èƒ½çœ‹åˆ° 6 å¼ å¡
    n_gpus = torch.cuda.device_count()
    print(f"[i] Detected GPUs: {n_gpus}")
    for i in range(n_gpus):
        free_m = torch.cuda.mem_get_info(i)[0]/1024**3
        print(f"    - GPU {i}: {torch.cuda.get_device_name(i)} | Free: {free_m:.2f} GB")

    # 2. åŠ è½½ Tokenizer
    tokenizer = AutoTokenizer.from_pretrained(MODEL_ID, use_fast=False)
    
    # 3. å¼ºåˆ¶ GPU åŠ è½½ï¼Œä¸¥ç¦ CPU Offloading
    print(f"[ğŸ“‚] Loading 140GB weights... Ensuring zero CPU-offload.")
    # è„šæœ¬ï¼šclinicalnlplab/me-llama
    model = AutoModelForCausalLM.from_pretrained(
        MODEL_ID,
        torch_dtype=torch.bfloat16, 
        device_map="auto",
        # æ ¸å¿ƒä¿®å¤ï¼šé˜²æ­¢ RAM è¢«æ’‘çˆ†
        low_cpu_mem_usage=True, 
        # å¼ºåˆ¶ GPU åˆ†ç‰‡ï¼Œé¢„ç•™æ˜¾å­˜ä½™é‡
        max_memory={i: "42GiB" for i in range(6)}, 
        trust_remote_code=True
    )

    # æ£€æŸ¥ Device Map æ˜¯å¦åŒ…å« CPU æˆ– Disk
    if any(v in ['cpu', 'disk'] for v in model.hf_device_map.values()):
        print("âš ï¸ è­¦å‘Šï¼šæ£€æµ‹åˆ° CPU/Disk å¸è½½ï¼æ¨ç†é€Ÿåº¦å°†ææ…¢ã€‚è¯·æ£€æŸ¥ GPU 0 ç©ºé—´ã€‚")
    else:
        print("[âœ…] All layers successfully mapped to GPUs.")

    num_params = model.num_parameters(only_trainable=False) #
    
    prompt = f"Instruction: Provide a professional clinical analysis.\nQuestion: {question}\nAnswer:"
    inputs = tokenizer(prompt, return_tensors="pt").to("cuda")
    input_len = inputs.input_ids.shape[1]
    
    streamer = TextStreamer(tokenizer, skip_prompt=True)

    print("\n" + "="*15 + " [ğŸ§  Inference Start] " + "="*15)
    
    # é‡ç½®æ‰€æœ‰å¡çš„æ˜¾å­˜å³°å€¼ç»Ÿè®¡
    for i in range(n_gpus):
        torch.cuda.reset_peak_memory_stats(i)
    
    torch.cuda.synchronize()
    start_time = time.perf_counter()

    with torch.no_grad():
        output = model.generate(
            **inputs, 
            max_new_tokens=128, 
            streamer=streamer, 
            do_sample=False,
            pad_token_id=tokenizer.eos_token_id
        )

    torch.cuda.synchronize()
    latency = time.perf_counter() - start_time
    print("\n" + "="*15 + " [ğŸ§  Inference End] " + "="*15)
    
    # --- æ ¸å¿ƒæŒ‡æ ‡ä¿®æ­£ ---
    # 1. æ±‡æ€» 6 å¼ å¡çš„å³°å€¼æ˜¾å­˜æ€»å’Œ
    total_peak_mem = sum(torch.cuda.max_memory_allocated(i) for i in range(n_gpus)) / (1024**3)
    
    out_tokens = output.shape[1] - input_len
    flops = 2 * num_params * (input_len + out_tokens)

    print(f"\nğŸ“Š Me-LLaMA-70B Rigorous Results:")
    print(f"   - System-wide Peak Memory: {total_peak_mem:.2f} GB") # è§£å†³ 21GB çš„ç»Ÿè®¡å¹»è§‰
    print(f"   - End-to-End Latency: {latency:.2f} s")
    print(f"   - Reasoning Speed: {out_tokens/latency:.2f} tokens/s")
    print(f"   - Total TFLOPs: {flops / 1e12:.2f}")

if __name__ == "__main__":
    # å…ˆæ¸…ç†ä¸€éåƒµå°¸è¿›ç¨‹
    import subprocess
    subprocess.run(["pkill", "-u", "yuqiao", "-9", "python"])
    
    run_mellama_benchmark("A patient with history of heavy smoking has progressive dyspnea and clubbing. Diagnosis?")