

# clinicalnlplab/me-llama
# meta-llama/Meta-Llama-3-70B-Instruct
# Qwen/Qwen2.5-72B-Instruct
import torch
import time
from transformers import AutoModelForCausalLM, AutoTokenizer, TextStreamer

MODEL_ID = "epfl-llm/meditron-70b"

def profile_comprehensive(question):
    print(f"[âš™ï¸] Loading {MODEL_ID} across your 6x RTX 6000 Ada...")
    tokenizer = AutoTokenizer.from_pretrained(MODEL_ID)
    
    # è‡ªåŠ¨åˆ‡åˆ†åˆ° 6 å¼  48GB å¡
    model = AutoModelForCausalLM.from_pretrained(
        MODEL_ID,
        torch_dtype=torch.bfloat16, 
        device_map="auto",
        trust_remote_code=True
    )

    # è·å–çœŸå®çš„å‚æ•°é‡ç”¨äº FLOPs è®¡ç®—
    # å»ºè®®åŠ ä¸Š only_trainable=False ä»¥è·å–å®Œæ•´çš„æ¨¡å‹å‚æ•°é‡
    num_params = model.num_parameters(only_trainable=False)
    
    prompt = f"Question: {question}\nAnswer:"
    inputs = tokenizer(prompt, return_tensors="pt").to("cuda")
    input_len = inputs.input_ids.shape[1]
    
    # é‡ç½®æ˜¾å­˜ç»Ÿè®¡
    torch.cuda.empty_cache()
    torch.cuda.reset_peak_memory_stats()
    
    # å¼•å…¥æµå¼è¾“å‡ºå™¨ï¼Œè®©ä½ ä¸å†å‚»ç­‰
    streamer = TextStreamer(tokenizer, skip_prompt=True)

    print("\n[âš¡] Starting Inference (Streaming Mode)...")
    torch.cuda.synchronize()
    start_time = time.perf_counter()

    with torch.no_grad():
        output = model.generate(
            **inputs, 
            max_new_tokens=128, 
            streamer=streamer, 
            do_sample=False,
            pad_token_id=tokenizer.eos_token_id
        )

    torch.cuda.synchronize()
    end_time = time.perf_counter()
    
    # --- æŒ‡æ ‡è®¡ç®— ---
    latency = end_time - start_time
    # ç»Ÿè®¡æ‰€æœ‰å¡ä¸­çš„å³°å€¼æ˜¾å­˜æ€»å’Œ
    peak_mem_gb = torch.cuda.max_memory_allocated() / (1024**3) 
    out_tokens = output.shape[1] - input_len
    
    # ç†è®º FLOPs: 2 * P * (N_in + N_out)
    total_tokens = input_len + out_tokens
    theoretical_flops = 2 * num_params * total_tokens

    print("\n" + "="*40)
    print(f"ğŸ“Š Final Profiling Results for {MODEL_ID}:")
    print(f"   - Peak Memory (System-wide): {peak_mem_gb:.2f} GB")
    print(f"   - End-to-End Latency: {latency:.2f} s")
    print(f"   - Throughput: {out_tokens/latency:.2f} tokens/s")
    print(f"   - Theoretical Computation: {theoretical_flops / 1e12:.2f} TFLOPs")
    print("="*40)

if __name__ == "__main__":
    sample = "A 46-year-old man presents with progressive shortness of breath. What is the most likely diagnosis?"
    profile_comprehensive(sample)