import torch
import time
import sys
from tqdm import tqdm
from transformers import AutoModelForCausalLM, AutoTokenizer, TextStreamer

# ç›®æ ‡æ¨¡å‹ï¼šQwen-2.5-72B
MODEL_ID = "Qwen/Qwen2.5-72B-Instruct"

def profile_with_progress(question):
    print(f"\n[ğŸš€] Initializing Deployment Suite for: {MODEL_ID}")
    
    # 1. åŠ è½½ Tokenizer
    tokenizer = AutoTokenizer.from_pretrained(MODEL_ID, trust_remote_code=True)
    
    # 2. åˆ†å¸ƒå¼åŠ è½½æ¨¡å‹ï¼ˆå¸¦è¿›åº¦æ¡æ¨¡æ‹Ÿï¼‰
    # æ³¨æ„ï¼šHuggingFace åŸç”Ÿæ”¯æŒ shard åŠ è½½è¿›åº¦ï¼Œæˆ‘ä»¬é€šè¿‡ print æ˜ç¡®é˜¶æ®µ
    print(f"[ğŸ“‚] Loading model weights into 6x RTX 6000 Ada VRAM... (Approx. 145GB)")
    start_load = time.perf_counter()
    
    model = AutoModelForCausalLM.from_pretrained(
        MODEL_ID,
        torch_dtype=torch.bfloat16, 
        device_map="auto",
        trust_remote_code=True,
        low_cpu_mem_usage=True
    )
    load_time = time.perf_counter() - start_load
    print(f"[âœ…] Model loaded successfully in {load_time:.2f}s")

    # 3. åŠ¨æ€è·å–å‚æ•°é‡
    num_params = model.num_parameters(only_trainable=False)
    
    # 4. æ„é€  Prompt
    prompt = f"<|im_start|>system\nYou are a medical expert assistant.<|im_end|>\n<|im_start|>user\n{question}<|im_end|>\n<|im_start|>assistant\n"
    inputs = tokenizer(prompt, return_tensors="pt").to("cuda")
    input_len = inputs.input_ids.shape[1]
    
    # 5. å‡†å¤‡æ¨ç†æŒ‡æ ‡
    torch.cuda.empty_cache()
    torch.cuda.reset_peak_memory_stats()
    
    # å¼•å…¥æµå¼è¾“å‡ºå™¨ï¼Œè§£å†³â€œç­‰å¾—æªå¿ƒâ€çš„é—®é¢˜
    streamer = TextStreamer(tokenizer, skip_prompt=True)

    print("\n" + "-"*20 + " [ğŸ§  Reasoning Start] " + "-"*20)
    
    torch.cuda.synchronize()
    start_inf = time.perf_counter()

    # æ‰§è¡Œç”Ÿæˆ
    with torch.no_grad():
        output = model.generate(
            **inputs, 
            max_new_tokens=128, 
            streamer=streamer, 
            do_sample=False,
            pad_token_id=tokenizer.eos_token_id
        )

    torch.cuda.synchronize()
    inf_latency = time.perf_counter() - start_inf
    print("\n" + "-"*20 + " [ğŸ§  Reasoning End] " + "-"*20)
    
    # --- æŒ‡æ ‡è®¡ç®— (ç”¨äº Appendix æ•°æ®æ”¯æ’‘) ---
    peak_mem_gb = torch.cuda.max_memory_allocated() / (1024**3) 
    out_tokens = output.shape[1] - input_len
    
    # ç†è®ºè®¡ç®—é‡å…¬å¼:
    # $$FLOPs \approx 2 \times P \times (N_{in} + N_{out})$$
    total_tokens = input_len + out_tokens
    theoretical_flops = 2 * num_params * total_tokens

    print(f"\nğŸ“Š Deployment Metrics:")
    print(f"   - Peak Memory (Total): {peak_mem_gb:.2f} GB")
    print(f"   - Total Latency: {inf_latency:.2f} s")
    print(f"   - Reasoning Speed: {out_tokens/inf_latency:.2f} tokens/s")
    print(f"   - Computation Cost: {theoretical_flops / 1e12:.2f} TFLOPs")

if __name__ == "__main__":
    # é’ˆå¯¹ MedQA æˆ– NEJM çš„å…¸å‹ä¸´åºŠæ¡ˆä¾‹
    sample_q = "A 46-year-old male presents with progressive shortness of breath. Physical exam shows decreased breath sounds. Most likely diagnosis?"
    profile_with_progress(sample_q)